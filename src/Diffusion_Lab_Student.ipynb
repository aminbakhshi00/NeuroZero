{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b40d478c",
   "metadata": {},
   "source": [
    "# Diffusion Lab\n",
    "Welcome to the student workbook for the diffusion lab. Each section mirrors the instructor notebook but leaves intentional gaps so you can reason about them. Use the provided hints, experiment often, and take notes about what surprised you.\n",
    "\n",
    "> **Expectation:** You may consult AI tools, but you must understand and explain every line you accept. Write short reflections in the provided cells to solidify your learning. Some parts of the lab may be challenging; embrace the struggle as part of the learning process!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c172799",
   "metadata": {},
   "source": [
    "## Rules & Notes\n",
    "- **Libraries:** Only use **NumPy** and **Matplotlib**.\n",
    "- **Keep it simple:** Small dataset, small network, clear visuals.\n",
    "- **Focus on understanding:** Read comments, run cells one-by-one, and observe the plots.\n",
    "- **Workflow tip:** When you see **TODO**, stop and think before coding; if you get stuck, document what you tried before asking for help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6003d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility helpers\n",
    "RNG_SEED = 42\n",
    "rng = np.random.default_rng(RNG_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4875529",
   "metadata": {},
   "source": [
    "## 1) Build a small 2D toy dataset (a spiral)\n",
    "We want a 2D dataset that makes it easy to **see** whether denoising works. A noisy spiral does the trick.\n",
    "\n",
    "**TODO:** Complete `make_spiral`. Keep it simple: sample an angle `t`, map to `(x, y)`, add gentle Gaussian jitter, and normalize.\n",
    "\n",
    "> **Hint:** The instructor version scales the radius linearly with the angle and then standardizes the data. You can mimic that or invent your own variation just document it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7f8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spiral(n_samples=1000, noise=0.02):\n",
    "    \"\"\"Return a normalized (n_samples, 2) array shaped like a spiral.\"\"\"\n",
    "    # TODO: implement using rng for randomness\n",
    "    raise NotImplementedError(\"Fill in make_spiral to proceed.\")\n",
    "\n",
    "X = make_spiral()\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(X[:,0], X[:,1], s=5)\n",
    "plt.title(\"Spiral dataset (normalized)\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bfb25e",
   "metadata": {},
   "source": [
    "## 2) Make a simple **noise schedule** (linear $\\beta_t$)\n",
    "A **noise schedule** tells us how much Gaussian noise to inject at each diffusion step. In DDPM we typically choose a simple schedule such as a linear ramp so that early steps barely disturb $x_0$ while later steps push the sample toward pure noise. The closed-form forward equation\n",
    "$$ x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\, \\epsilon $$\n",
    "depends on the cumulative product $\\bar{\\alpha}_t = \\prod_{s=1}^t (1-\\beta_s)$, so carefully shaping $\\beta_t$ directly controls how fast signal fades.\n",
    "\n",
    "**TODOs:**\n",
    "1. Implement `make_linear_beta_schedule` so it returns `betas`, `alphas`, and the cumulative product `alpha_bars`.\n",
    "2. Plot the schedule to see if it matches your intuition.\n",
    "\n",
    "> **Hint:** Remember `alpha_t = 1 - beta_t`. Use `np.cumprod` for the running product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d095f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_linear_beta_schedule(T=100, beta_start=1e-4, beta_end=5e-3):\n",
    "    \"\"\"Return betas, alphas, and cumulative alpha_bars for T steps.\"\"\"\n",
    "    # TODO: fill in this function using np.linspace and np.cumprod\n",
    "    raise NotImplementedError\n",
    "\n",
    "T = 100\n",
    "betas, alphas, alpha_bars = make_linear_beta_schedule(T=T)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(betas)\n",
    "plt.title(\"Linear beta schedule\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"beta_t\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(alpha_bars)\n",
    "plt.title(\"Cumulative product: alpha_bar_t\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"alpha_bar_t\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fab631",
   "metadata": {},
   "source": [
    "## 3) Forward diffusion: make $x_t$ from $x_0$\n",
    "The closed-form equation lets us jump directly to any timestep.\n",
    "\n",
    "**TODO:** Finish `q_sample` so it returns `(x_t, eps)` for either a scalar `t` or a vector of times. For the equation, go back to\n",
    "$$ x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\, \\epsilon. $$\n",
    "\n",
    "> **Hint:** When `t` is a vector, you'll want to broadcast `alpha_bars[t-1]` to match the shape of `x0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d30dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_sample(x0, t, alpha_bars):\n",
    "    \"\"\"Sample x_t and record the actual noise epsilon.\"\"\"\n",
    "    # TODO: support scalar or vector t values\n",
    "    raise NotImplementedError\n",
    "    \n",
    "# Visualize noising at different t values\n",
    "ts_to_show = [1, T//4, T//2, T]\n",
    "fig, axes = plt.subplots(1, len(ts_to_show), figsize=(12,3))\n",
    "for i, tt in enumerate(ts_to_show):\n",
    "    xt, _ = q_sample(X, tt, alpha_bars)\n",
    "    axes[i].scatter(xt[:,0], xt[:,1], s=5)\n",
    "    axes[i].set_title(f\"t={tt}\")\n",
    "    axes[i].set_aspect('equal', 'box')\n",
    "plt.suptitle(\"Forward diffusion: adding noise over time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d5447c",
   "metadata": {},
   "source": [
    "## 4) A multi layer network to predict the noise $\\hat{\\epsilon}$\n",
    "In DDPM we train a model to estimate the noise $\\epsilon$ injected during the forward process. If a network can **recover that noise** from $x_t$ and the current timestep $t$, the reverse sampler can subtract it and gradually turn noise back into data.\n",
    "\n",
    "Remember from Section 1 that you normalized the spiral so each feature had roughly unit scale. That choice strongly influences how large your initial weights should be. If the data is standardized, smaller ranges often keep activations in a sensible region. Keep this connection in mind while you tune the network.\n",
    "\n",
    "- $x_t, y_t$: the two coordinates of the noised point at time $t$ (the input the network sees).\n",
    "- $t/T$: the normalized scalar timestep so the model knows how much noise to expect.\n",
    "\n",
    "**TODO:** Implement the linear layers + ReLU by replacing the `raise NotImplementedError` blocks in `forward_network` and `backward_network`, and justify your chosen values for `WEIGHT_INITIALIZATION_RANGE` and `DEFAULT_MOMENTUM`. Explain how these choices interact with your data normalization and whether momentum is truly required.\n",
    "\n",
    "> **Hint:** This is a plain 3-layer MLP. Start from matrix multiplication + bias addition + ReLU, nothing fancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e572a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set these after experimenting with normalization + training stability\n",
    "WEIGHT_INITIALIZATION_RANGE = None\n",
    "DEFAULT_MOMENTUM = None\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0.0, x)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return (x > 0).astype(np.float32)\n",
    "\n",
    "def MultilayerNetwork(input_dim=3, hidden=128, out_dim=2, rng=rng, momentum=DEFAULT_MOMENTUM):\n",
    "    if WEIGHT_INITIALIZATION_RANGE is None or momentum is None:\n",
    "        raise ValueError(\"Set WEIGHT_INITIALIZATION_RANGE and DEFAULT_MOMENTUM after investigating their effects.\")\n",
    "    net = {\n",
    "        \"W1\": rng.normal(0, WEIGHT_INITIALIZATION_RANGE, size=(input_dim, hidden)).astype(np.float32),\n",
    "        \"b1\": np.zeros(hidden, dtype=np.float32),\n",
    "        \"W2\": rng.normal(0, WEIGHT_INITIALIZATION_RANGE, size=(hidden, hidden)).astype(np.float32),\n",
    "        \"b2\": np.zeros(hidden, dtype=np.float32),\n",
    "        \"W3\": rng.normal(0, WEIGHT_INITIALIZATION_RANGE, size=(hidden, out_dim)).astype(np.float32),\n",
    "        \"b3\": np.zeros(out_dim, dtype=np.float32),\n",
    "        \"momentum\": momentum,\n",
    "    }\n",
    "    return net\n",
    "\n",
    "def forward_network(net, x):\n",
    "    z1 = x @ net[\"W1\"] + net[\"b1\"]\n",
    "    h1 = relu(z1)\n",
    "    z2 = h1 @ net[\"W2\"] + net[\"b2\"]\n",
    "    h2 = relu(z2)\n",
    "    out = h2 @ net[\"W3\"] + net[\"b3\"]\n",
    "    cache = (x, z1, h1, z2, h2)\n",
    "    return out, cache\n",
    "\n",
    "def backward_network(net, cache, grad_out, lr=1e-3):\n",
    "    x, z1, h1, z2, h2 = cache\n",
    "    dW3 = h2.T @ grad_out\n",
    "    db3 = grad_out.sum(axis=0)\n",
    "    dh2 = grad_out @ net[\"W3\"].T\n",
    "    dz2 = dh2 * relu_deriv(z2)\n",
    "    dW2 = h1.T @ dz2\n",
    "    db2 = dz2.sum(axis=0)\n",
    "    dh1 = dz2 @ net[\"W2\"].T\n",
    "    dz1 = dh1 * relu_deriv(z1)\n",
    "    dW1 = x.T @ dz1\n",
    "    db1 = dz1.sum(axis=0)\n",
    "    grads = {\n",
    "        \"W3\": dW3,\n",
    "        \"b3\": db3,\n",
    "        \"W2\": dW2,\n",
    "        \"b2\": db2,\n",
    "        \"W1\": dW1,\n",
    "        \"b1\": db1,\n",
    "    }\n",
    "    for name, grad in grads.items():\n",
    "        net[name] -= lr * net[\"momentum\"] * grad\n",
    "\n",
    "def mse_loss(pred, target):\n",
    "    diff = pred - target\n",
    "    return np.mean(diff**2), (2.0 / pred.shape[0]) * diff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdfbd79",
   "metadata": {},
   "source": [
    "## 5) Train to predict noise\n",
    "Now that you can sample $x_t$ and build a small network, stitch everything together. Each iteration should (1) use the whole dataset, (2) pick random timesteps, (3) generate $x_t$ with `q_sample`, (4) feed `[x_t, t/T]` through the network, and (5) measure the MSE between predicted and true noise. Track a smoothed loss to see trends.\n",
    "\n",
    "**TODO:** Write the entire training loop yourself. No starter code is provided below. Describe and justify each design choice (learning rate, smoothing, momentum) in comments or short markdown notes. Why is your choice appropriate for this spiral dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e20e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MultilayerNetwork(input_dim=3, hidden=128, out_dim=2, rng=rng , momentum=DEFAULT_MOMENTUM)\n",
    "\n",
    "def train(network, X, T, alpha_bars, steps, lr):\n",
    "    # TODO: implement the full steepest-descent routine described above\n",
    "    pass\n",
    "\n",
    "# TODO: choose reasonable defaults before running\n",
    "\n",
    "losses = train(network, X, T, alpha_bars, steps=?, lr=?)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training loss (steepest descent)\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ec73f0",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "What happens if you reduce the momentum dramatically (e.g., from 0.999 to 0.5)? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3288c2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection = \"TODO: discuss smoothing behavior.\"\n",
    "print(reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474f5478",
   "metadata": {},
   "source": [
    "## 6) Sampling: generate new data by reversing diffusion\n",
    "The function $p_\theta(x_{t-1}\\mid x_t)$ is the learned reverse transition: given a noisy sample $x_t$, we predict the noise $\\hat{\\epsilon}$ and use it to recover a denoised estimate $x_{t-1}$. In a DDPM this is the core step that turns pure Gaussian noise back into data, so implementing `p_sample` correctly is essential.\n",
    "\n",
    "Now use your trained network to walk backward from pure Gaussian noise to the data manifold. Every reverse step should:\n",
    "1. Build the same conditioning vector `[x_t, t/T]` you used during training.\n",
    "2. Predict the noise $\\hat{\\epsilon}$ and plug it into the DDPM update equation.\n",
    "3. Optionally add fresh Gaussian noise unless you're at $t=1$.\n",
    "\n",
    "**TODOs:(estimate 2 hours)**\n",
    "- Fill in the missing math inside `p_sample` (mean term and the optional noise injection).\n",
    "- Reuse the instructor-style `p_sample_loop` to log snapshots at the times in `ts_to_show` so you can compare with the forward plots.\n",
    "- Explain in markdown how the snapshots confirm (or contradict) your expectations about the denoising path.\n",
    "\n",
    "> **Hint:** $\\sqrt{\\bar{\\alpha}_t}$ and $\\sqrt{1-\\bar{\\alpha}_t}$ were already computed in the forward helper, use them again here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311355e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_sample(network, x_t, t, betas, alphas, alpha_bars):\n",
    "    t_scaled = np.full((x_t.shape[0],1), t/len(betas), dtype=np.float32)\n",
    "    net_in = np.concatenate([x_t, t_scaled], axis=1)\n",
    "    eps_hat, _ = forward_network(network, net_in)\n",
    "\n",
    "    a_t = alphas[t-1]\n",
    "    ab_t = alpha_bars[t-1]\n",
    "    beta_t = betas[t-1]\n",
    "\n",
    "    # TODO: derive the DDPM mean using eps_hat (see Section 6 equation)\n",
    "    mean_part = None\n",
    "    raise NotImplementedError(\"Replace with the correct mean expression.\")\n",
    "\n",
    "    if t > 1:\n",
    "        z = rng.normal(0.0, 1.0, size=x_t.shape).astype(np.float32)\n",
    "        sigma = np.sqrt(beta_t)\n",
    "        # TODO: combine the mean with sigma * z\n",
    "        raise NotImplementedError(\"Combine mean and stochastic term when t>1.\")\n",
    "    else:\n",
    "        x_prev = mean_part\n",
    "    return x_prev\n",
    "\n",
    "def p_sample_loop(network, n_samples, T, betas, alphas, alpha_bars, ts_to_show=None):\n",
    "    x_t = rng.normal(0.0, 1.0, size=(n_samples, 2)).astype(np.float32)\n",
    "    for t in range(T, 0, -1):\n",
    "        x_t = p_sample(network, x_t, t, betas, alphas, alpha_bars)\n",
    "        if ts_to_show is not None and t in ts_to_show:\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.scatter(X[:,0], X[:,1], s=5, label=\"real\")\n",
    "            plt.scatter(x_t[:,0], x_t[:,1], s=5, label=\"generated\")\n",
    "            plt.title(f\"Generated samples at t={t}\")\n",
    "            plt.axis(\"equal\")\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "    return x_t\n",
    "\n",
    "ts_to_show = [1, T//4, T//2, T]\n",
    "gen = p_sample_loop(network, n_samples=1000, T=T, betas=betas, alphas=alphas, alpha_bars=alpha_bars, ts_to_show=ts_to_show)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(X[:,0], X[:,1], s=5, label=\"real\")\n",
    "plt.scatter(gen[:,0], gen[:,1], s=5, label=\"generated\")\n",
    "plt.legend()\n",
    "plt.title(\"Final Real vs Generated (simple DDPM)\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c9fb6",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "Compare the visual storytelling between the forward and reverse plots. What does each snapshot teach you about the diffusion process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f8f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection = \"TODO: describe insights from the reverse snapshots.\"\n",
    "print(reflection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721dd365",
   "metadata": {},
   "source": [
    "## 7) Discussion & Experiments\n",
    "Run at least two mini-investigations and jot down short answers:\n",
    "1. **Noise schedule tweak:** What happens to sample quality if you halve or double the final $\\beta_T$? Why?\n",
    "2. **Network capacity:** Does increasing the hidden width improve denoising, or does it overfit? Provide evidence.\n",
    "3. **Sampler behavior:** Try removing the stochastic term in `p_sample` for the last few steps. How does that change the visuals?\n",
    "4. **Failure analysis:** Capture one failure case (e.g., mode collapse) and explain what you think caused it.\n",
    "\n",
    "Conclude with one concrete lesson about diffusion models that you didn't appreciate before running these experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37211e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_report = \"TODO: summarize experiments and lessons learned.\"\n",
    "print(lab_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NeuroZero)",
   "language": "python",
   "name": "neurozero"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
