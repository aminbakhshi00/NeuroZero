{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b40d478c",
   "metadata": {},
   "source": [
    "# Minimal Diffusion Lab (Student Edition)\n",
    "Welcome! You'll build a diffusion model step-by-step. Each section mirrors the instructor solution but intentionally leaves key pieces out so you can reason about them. Use the provided hints, experiment often, and take notes about what surprised you.\n",
    "\n",
    "> **Expectation:** You may consult AI tools, but you must understand and explain every line you accept. Write short reflections in the provided cells to solidify your learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c172799",
   "metadata": {},
   "source": [
    "## 0. Lab Survival Kit\n",
    "- Work top-to-bottom, running cells one at a time.\n",
    "- When you see **TODO**, stop and think before coding.\n",
    "- Use the **Reflection** prompts; they're graded.\n",
    "- If you get stuck, describe what you tried and why it failed before asking for help (human or AI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6003d7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility helpers\n",
    "RNG_SEED = 42\n",
    "rng = np.random.default_rng(RNG_SEED)\n",
    "print(\"✅ Imports ready. Seed:\", RNG_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4875529",
   "metadata": {},
   "source": [
    "## 1. Build a toy spiral dataset\n",
    "We want a 2D dataset that makes it easy to **see** whether denoising works. A noisy spiral does the trick.\n",
    "\n",
    "**TODO: (estimate 30 min)** Complete `make_spiral`. Keep it simple: sample an angle `t`, map to `(x, y)`, add gentle Gaussian jitter, and normalize.\n",
    "\n",
    "> **Hint:** The instructor version scales the radius linearly with the angle and then standardizes the data. You can mimic that or invent your own variation—just document it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7f8a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spiral(n_samples=1000, noise=0.02):\n",
    "    \"\"\"Return a normalized (n_samples, 2) array shaped like a spiral.\"\"\"\n",
    "    # TODO: implement using rng for randomness\n",
    "    raise NotImplementedError(\"Fill in make_spiral to proceed.\")\n",
    "\n",
    "X = make_spiral()\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(X[:,0], X[:,1], s=5)\n",
    "plt.title(\"Spiral dataset (normalized)\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bfb25e",
   "metadata": {},
   "source": [
    "## 2. Noise schedule $\\beta_t$\n",
    "A **noise schedule** tells us how much Gaussian noise to inject at each diffusion step. In DDPM we typically choose a simple schedule such as a linear ramp so that early steps barely disturb $x_0$ while later steps push the sample toward pure noise. The closed-form forward equation\n",
    "$$ x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\, \\epsilon $$\n",
    "depends on the cumulative product $\\bar{\\alpha}_t = \\prod_{s=1}^t (1-\\beta_s)$, so carefully shaping $\\beta_t$ directly controls how fast signal fades.\n",
    "\n",
    "**TODOs:(estimate 20 min)**\n",
    "1. Implement `make_linear_beta_schedule` so it returns `betas`, `alphas`, and the cumulative product `alpha_bars`.\n",
    "2. Plot the schedule to see if it matches your intuition.\n",
    "\n",
    "> **Hint:** Remember `alpha_t = 1 - beta_t`. Use `np.cumprod` for the running product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d095f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_linear_beta_schedule(T=100, beta_start=1e-4, beta_end=5e-3):\n",
    "    \"\"\"Return betas, alphas, and cumulative alpha_bars for T steps.\"\"\"\n",
    "    # TODO: fill in this function using np.linspace and np.cumprod\n",
    "    raise NotImplementedError\n",
    "\n",
    "T = 100\n",
    "betas, alphas, alpha_bars = make_linear_beta_schedule(T=T)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(betas)\n",
    "plt.title(\"Linear beta schedule\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"beta_t\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(alpha_bars)\n",
    "plt.title(\"Cumulative product: alpha_bar_t\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"alpha_bar_t\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fab631",
   "metadata": {},
   "source": [
    "## 3. Forward diffusion helper $q(x_t \\mid x_0)$\n",
    "The closed-form equation lets us jump directly to any timestep.\n",
    "\n",
    "**TODO:(estimate 10 min)** Finish `q_sample` so it returns `(x_t, eps)` for either a scalar `t` or a vector of times. For the equation, go back to \n",
    "$ x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\, \\epsilon $. \n",
    "\n",
    "> **Hint:** When `t` is a vector, you'll want to broadcast `alpha_bars[t-1]` to match the shape of `x0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d30dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_sample(x0, t, alpha_bars):\n",
    "    \"\"\"Sample x_t and record the actual noise epsilon.\"\"\"\n",
    "    # TODO: support scalar or vector t values\n",
    "    raise NotImplementedError\n",
    "    \n",
    "# Visualize noising at different t values\n",
    "ts_to_show = [1, T//4, T//2, T]\n",
    "fig, axes = plt.subplots(1, len(ts_to_show), figsize=(12,3))\n",
    "for i, tt in enumerate(ts_to_show):\n",
    "    xt, _ = q_sample(X, tt, alpha_bars)\n",
    "    axes[i].scatter(xt[:,0], xt[:,1], s=5)\n",
    "    axes[i].set_title(f\"t={tt}\")\n",
    "    axes[i].set_aspect('equal', 'box')\n",
    "plt.suptitle(\"Forward diffusion: adding noise over time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d5447c",
   "metadata": {},
   "source": [
    "## 4. Multi layer neural network to predict noise\n",
    "We'll train a small fully connected network that takes $[x_t, y_t, t/T]$ and predicts the noise $\\epsilon$ that was added.\n",
    "\n",
    "### TODOs inside the network module\n",
    "1. Replace the `raise NotImplementedError` statements in `forward_network` and `backward_network` with actual linear + ReLU math.\n",
    "2. Momentum is provided for you; just update the velocity buffers.\n",
    "\n",
    "> **Hint:** This is a plain 3-layer MLP. Start from matrix multiplication + bias addition + ReLU, nothing fancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e572a54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0.0, x)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return (x > 0).astype(np.float32)\n",
    "\n",
    "def make_network(in_dim=3, hidden=128, out_dim=2, rng=rng, scale=0.02, momentum=0.999):\n",
    "    net = {\n",
    "        \"W1\": rng.normal(0.0, scale, size=(in_dim, hidden)).astype(np.float32),\n",
    "        \"b1\": np.zeros(hidden, dtype=np.float32),\n",
    "        \"W2\": rng.normal(0.0, scale, size=(hidden, hidden)).astype(np.float32),\n",
    "        \"b2\": np.zeros(hidden, dtype=np.float32),\n",
    "        \"W3\": rng.normal(0.0, scale, size=(hidden, out_dim)).astype(np.float32),\n",
    "        \"b3\": np.zeros(out_dim, dtype=np.float32),\n",
    "        \"momentum\": momentum,\n",
    "    }\n",
    "    net[\"velocities\"] = {\n",
    "        \"W1\": np.zeros_like(net[\"W1\"]),\n",
    "        \"b1\": np.zeros_like(net[\"b1\"]),\n",
    "        \"W2\": np.zeros_like(net[\"W2\"]),\n",
    "        \"b2\": np.zeros_like(net[\"b2\"]),\n",
    "        \"W3\": np.zeros_like(net[\"W3\"]),\n",
    "        \"b3\": np.zeros_like(net[\"b3\"]),\n",
    "    }\n",
    "    return net\n",
    "\n",
    "def forward_network(net, x):\n",
    "    # TODO: compute the forward pass and return (out, cache)\n",
    "    raise NotImplementedError\n",
    "\n",
    "def backward_network(net, cache, grad_out, lr=1e-3):\n",
    "    # TODO: backprop through the network and apply SGD + momentum\n",
    "    raise NotImplementedError\n",
    "\n",
    "def mse_loss(pred, target):\n",
    "    diff = pred - target\n",
    "    return np.mean(diff**2), (2.0 / pred.shape[0]) * diff\n",
    "\n",
    "network = make_network()\n",
    "print(\"Network layers:\", [k for k in network.keys() if not k.startswith('velocity')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdfbd79",
   "metadata": {},
   "source": [
    "## 5. Train to predict noise\n",
    "We'll use full-dataset steps (a.k.a. true steepest descent) for clarity. Every update:\n",
    "1. Uses all samples in `X`.\n",
    "2. Draws a random timestep for each sample.\n",
    "3. Generates `x_t` with `q_sample`.\n",
    "4. Feeds `[x_t, t/T]` into the network.\n",
    "5. Minimizes MSE between predicted and true noise.\n",
    "\n",
    "**TODOs inside `train`:**\n",
    "- Replace the `pass` statements with actual training logic.\n",
    "- Track an exponential moving average of the loss for smoother plotting.\n",
    "\n",
    "> **Hint:** Most of the pieces are already written in previous sections—reuse them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e20e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, X, T, betas, alphas, alpha_bars, steps=8000, lr=5e-2, smoothing=0.999):\n",
    "    losses = []\n",
    "    running = None\n",
    "    for step in range(1, steps+1):\n",
    "        # TODO: implement the training loop described above\n",
    "        pass\n",
    "    return np.array(losses)\n",
    "\n",
    "losses = train(network, X, T, betas, alphas, alpha_bars)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training loss (EMA)\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ec73f0",
   "metadata": {},
   "source": [
    "### Reflection 2\n",
    "What happens if you reduce the smoothing factor dramatically (e.g., from 0.999 to 0.5)? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3288c2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_2 = \"TODO: discuss smoothing behavior.\"\n",
    "print(reflection_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474f5478",
   "metadata": {},
   "source": [
    "## 6. Sampling (reverse diffusion)\n",
    "With a trained network we can denoise from pure noise back to data.\n",
    "\n",
    "**TODOs:**\n",
    "- Implement `p_sample` based on the DDPM update equation in the markdown above.\n",
    "- Use `p_sample_loop` to track snapshots at the timesteps in `ts_to_show` so you can visualize the denoising trajectory.\n",
    "\n",
    "> **Hint:** The forward helper already computed $\\sqrt{\bar{\u0007lpha}_t}$ and $\\sqrt{1-\bar{\u0007lpha}_t}$. Reuse those patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311355e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_sample(network, x_t, t, betas, alphas, alpha_bars):\n",
    "    # TODO: implement reverse diffusion step\n",
    "    raise NotImplementedError\n",
    "\n",
    "def p_sample_loop(network, n_samples, T, betas, alphas, alpha_bars, ts_to_save=None):\n",
    "    x_t = rng.normal(0.0, 1.0, size=(n_samples, 2)).astype(np.float32)\n",
    "    snapshots = {}\n",
    "    save_set = set(ts_to_save or [])\n",
    "    for t in range(T, 0, -1):\n",
    "        if t in save_set:\n",
    "            snapshots[t] = x_t.copy()\n",
    "        x_t = p_sample(network, x_t, t, betas, alphas, alpha_bars)\n",
    "    snapshots[0] = x_t\n",
    "    return x_t, snapshots\n",
    "\n",
    "ts_to_show = [1, T//4, T//2, T]\n",
    "gen, reverse_snaps = p_sample_loop(network, n_samples=1000, T=T, betas=betas, alphas=alphas, alpha_bars=alpha_bars, ts_to_save=ts_to_show)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(X[:,0], X[:,1], s=5, label=\"real\")\n",
    "plt.scatter(gen[:,0], gen[:,1], s=5, label=\"generated\")\n",
    "plt.legend()\n",
    "plt.title(\"Real vs Generated (simple DDPM)\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()\n",
    "\n",
    "fig, axes = plt.subplots(1, len(ts_to_show), figsize=(12,3))\n",
    "for i, tt in enumerate(ts_to_show):\n",
    "    pts = reverse_snaps.get(tt)\n",
    "    axes[i].scatter(pts[:,0], pts[:,1], s=5)\n",
    "    axes[i].set_title(f\"t={tt}\")\n",
    "    axes[i].set_aspect('equal', 'box')\n",
    "plt.suptitle(\"Reverse diffusion snapshots\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c9fb6",
   "metadata": {},
   "source": [
    "### Reflection 3\n",
    "Compare the visual storytelling between the forward and reverse plots. What does each snapshot teach you about the diffusion process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f8f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_3 = \"TODO: describe insights from the reverse snapshots.\"\n",
    "print(reflection_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721dd365",
   "metadata": {},
   "source": [
    "## 7. Experiments & reporting\n",
    "Use this space to describe at least two experiments you ran (schedule tweaks, network size changes, etc.) and what you learned from each. Screenshots or plots are encouraged.\n",
    "\n",
    "- Experiment A:\n",
    "- Experiment B:\n",
    "\n",
    "Wrap up with a concrete lesson learned about diffusion models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37211e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_report = \"TODO: summarize experiments and lessons learned.\"\n",
    "print(lab_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
