{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Minimal Diffusion Lab (Beginner-Friendly)\n",
    "\n",
    "**Goal:** Learn diffusion by **building a tiny model from scratch** using only **NumPy** and **Matplotlib**.\n",
    "\n",
    "**What you'll do:**\n",
    "1. Make a simple 2D toy dataset (a spiral).\n",
    "2. Add Gaussian noise step-by-step (forward diffusion).\n",
    "3. Train a tiny neural network to predict the noise (denoising).\n",
    "4. Generate new samples by reversing diffusion (sampling).\n",
    "5. Visualize everything and compare to the original data.\n",
    "\n",
    "> Time budget: ~5 hours. Keep the code simple and readable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00b2a0a",
   "metadata": {},
   "source": [
    "\n",
    "## Rules & Notes\n",
    "- **Libraries:** Only use **NumPy** and **Matplotlib**.\n",
    "- **Keep it simple:** Small dataset, tiny network, clear visuals.\n",
    "- **Focus on understanding:** Read comments, run cells one-by-one, and observe the plots.\n",
    "- **Notation & idea:** We follow the core DDPM idea: train a network to predict noise $\\epsilon$ added during forward diffusion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce760e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For reproducibility\n",
    "rng = np.random.default_rng(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30586569",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Build a tiny 2D toy dataset (a spiral)\n",
    "We'll generate a simple **2D spiral** because it's easy to see if our samples look right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f139a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_spiral(n_samples=1500, noise=0.02):\n",
    "    # Parameter t controls angle and radius\n",
    "    t = rng.uniform(0, 4*np.pi, size=(n_samples,))\n",
    "    # radius grows linearly with angle\n",
    "    r = t / (4*np.pi) * 1 \n",
    "    x = r * np.cos(t)\n",
    "    y = r * np.sin(t)\n",
    "    # small noise so it's not perfectly clean\n",
    "    x += rng.normal(0, noise, size=x.shape)\n",
    "    y += rng.normal(0, noise, size=y.shape)\n",
    "    data = np.stack([x, y], axis=1).astype(np.float32)\n",
    "    # Very important!: Normalize to roughly unit variance for stability \n",
    "    data = (data - data.mean(axis=0)) / (data.std(axis=0) + 1e-7)\n",
    "    return data\n",
    "\n",
    "X = make_spiral(n_samples=1500, noise=0.02)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(X[:,0], X[:,1], s=5)\n",
    "plt.title(\"Spiral dataset (normalized)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9283698",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Make a simple **noise schedule** (linear $\\beta_t$)\n",
    "We keep things tiny with just $T=100$ diffusion steps and linearly increase $\\beta_t$ from $10^{-4}$ to $5 \\times 10^{-3}$. That amount of noise is easy to grasp yet still teaches the reverse process something non-trivial.\n",
    "From $\\beta_t$ we derive:\n",
    "- $\\alpha_t = 1 - \\beta_t$\n",
    "- $\\bar{\\alpha}_t = \\prod_{s=1}^t \\alpha_s$\n",
    "\n",
    "We'll also plot the schedule to see how noise grows over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c46185a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T = 100\n",
    "\n",
    "def make_linear_beta_schedule(T=50, beta_start=1e-4, beta_end=5e-3):\n",
    "    betas = np.linspace(beta_start, beta_end, T, dtype=np.float32)\n",
    "    alphas = 1 - betas\n",
    "    alpha_bars = np.cumprod(alphas, axis=0)\n",
    "    return betas, alphas, alpha_bars\n",
    "\n",
    "betas, alphas, alpha_bars = make_linear_beta_schedule(T=T)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(betas)\n",
    "plt.title(\"Linear beta schedule\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"beta_t\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(alpha_bars)\n",
    "plt.title(\"Cumulative product: alpha_bar_t\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"alpha_bar_t\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4bb9c4",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Forward diffusion: make $x_t$ from $x_0$\n",
    "The formula:\n",
    "$$ x_t = \\sqrt{\\bar{\\alpha}_t}\\, x_0 + \\sqrt{1-\\bar{\\alpha}_t}\\, \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0,I) $$\n",
    "\n",
    "Let's write a helper to sample $x_t$ for any time $t$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b84f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def q_sample(x0, t, alpha_bars):\n",
    "    # t can be a scalar or array of integers in [1, T]\n",
    "    t_arr = np.asarray(t, dtype=np.int32)\n",
    "    if t_arr.ndim == 0:\n",
    "        ab = alpha_bars[int(t_arr) - 1]\n",
    "        mean = np.sqrt(ab) * x0\n",
    "        std = np.sqrt(1 - ab)\n",
    "    else:\n",
    "        ab = alpha_bars[t_arr - 1]\n",
    "        mean = np.sqrt(ab)[:, None] * x0\n",
    "        std = np.sqrt(1 - ab)[:, None]\n",
    "    eps = rng.normal(0, 1, size=x0.shape).astype(np.float32)\n",
    "    x_t = mean + std * eps\n",
    "    return x_t, eps\n",
    "\n",
    "# Visualize noising at different t values\n",
    "ts_to_show = [1, T//4, T//2, T]\n",
    "fig, axes = plt.subplots(1, len(ts_to_show), figsize=(12,3))\n",
    "for i, tt in enumerate(ts_to_show):\n",
    "    xt, _ = q_sample(X, tt, alpha_bars)\n",
    "    axes[i].scatter(xt[:,0], xt[:,1], s=5)\n",
    "    axes[i].set_title(f\"t={tt}\")\n",
    "    axes[i].set_aspect('equal', 'box')\n",
    "plt.suptitle(\"Forward diffusion: adding noise over time\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12bfc9b",
   "metadata": {},
   "source": [
    "\n",
    "## 4) A multi layer network to predict the noise $\\hat{\\epsilon}$\n",
    "Inputs: $[x_t, y_t, \\tilde{t}]$ where $\\tilde{t} = t/T$ (scaled time).  \n",
    "Outputs: $\\hat{\\epsilon}$ for the two coordinates.\n",
    "\n",
    "We'll write everything **from scratch**:\n",
    "- Linear layer: $y = xW + b$\n",
    "- ReLU: $\\max(0, x)$\n",
    "- MSE loss\n",
    "- Simple SGD updates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f84e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_deriv(x):\n",
    "    return (x > 0).astype(np.float32)\n",
    "\n",
    "def MultilayerNetwork(input_dim=3, hidden=128, out_dim=2, rng=rng,  momentum=0.999):\n",
    "    net = {\n",
    "        \"W1\": rng.normal(0, 0.02, size=(input_dim, hidden)).astype(np.float32),\n",
    "        \"b1\": np.zeros(hidden, dtype=np.float32),\n",
    "        \"W2\": rng.normal(0, 0.02, size=(hidden, hidden)).astype(np.float32),\n",
    "        \"b2\": np.zeros(hidden, dtype=np.float32),\n",
    "        \"W3\": rng.normal(0, 0.02, size=(hidden, out_dim)).astype(np.float32),\n",
    "        \"b3\": np.zeros(out_dim, dtype=np.float32),\n",
    "        \"momentum\": momentum,\n",
    "    }\n",
    "    net[\"velocities\"] = {\n",
    "        \"W1\": np.zeros_like(net[\"W1\"]),\n",
    "        \"b1\": np.zeros_like(net[\"b1\"]),\n",
    "        \"W2\": np.zeros_like(net[\"W2\"]),\n",
    "        \"b2\": np.zeros_like(net[\"b2\"]),\n",
    "        \"W3\": np.zeros_like(net[\"W3\"]),\n",
    "        \"b3\": np.zeros_like(net[\"b3\"]),\n",
    "    }\n",
    "    return net\n",
    "\n",
    "def forward_network(net, x):\n",
    "    z1 = x @ net[\"W1\"] + net[\"b1\"]\n",
    "    h1 = relu(z1)\n",
    "    z2 = h1 @ net[\"W2\"] + net[\"b2\"]\n",
    "    h2 = relu(z2)\n",
    "    out = h2 @ net[\"W3\"] + net[\"b3\"]\n",
    "    cache = (x, z1, h1, z2, h2)\n",
    "    return out, cache\n",
    "\n",
    "def backward_network(net, cache, grad_out, lr=1e-3):\n",
    "    x, z1, h1, z2, h2 = cache\n",
    "\n",
    "    dW3 = h2.T @ grad_out\n",
    "    db3 = grad_out.sum(axis=0)\n",
    "\n",
    "    dh2 = grad_out @ net[\"W3\"].T\n",
    "    dz2 = dh2 * relu_deriv(z2)\n",
    "\n",
    "    dW2 = h1.T @ dz2\n",
    "    db2 = dz2.sum(axis=0)\n",
    "\n",
    "    dh1 = dz2 @ net[\"W2\"].T\n",
    "    dz1 = dh1 * relu_deriv(z1)\n",
    "\n",
    "    dW1 = x.T @ dz1\n",
    "    db1 = dz1.sum(axis=0)\n",
    "\n",
    "    grads = {\n",
    "        \"W3\": dW3,\n",
    "        \"b3\": db3,\n",
    "        \"W2\": dW2,\n",
    "        \"b2\": db2,\n",
    "        \"W1\": dW1,\n",
    "        \"b1\": db1,\n",
    "    }\n",
    "\n",
    "    for name, grad in grads.items():\n",
    "        vel = net[\"velocities\"][name]\n",
    "        vel = net[\"momentum\"] * vel - lr * grad\n",
    "        net[\"velocities\"][name] = vel\n",
    "        net[name] += vel\n",
    "\n",
    "def mse_loss(pred, target):\n",
    "    diff = pred - target\n",
    "    mse_value = np.mean(diff**2)\n",
    "    # pred.shape[0] is the number of samples\n",
    "    return mse_value, (2* diff / pred.shape[0])  # loss, grad pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaf6d17",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Train to predict noise\n",
    "Each update still uses the entire dataset, but now the code avoids classes and is written in a very direct style.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f22ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "network = MultilayerNetwork(input_dim=3, hidden=128, out_dim=2, rng=rng , momentum=0.999)\n",
    "\n",
    "def train(network, X, T, betas, alphas, alpha_bars, steps=8000, lr=5e-2):\n",
    "    losses = []\n",
    "    running = None\n",
    "    for step in range(1, steps+1):\n",
    "        x0 = X\n",
    "        t = rng.integers(1, T+1, size=(x0.shape[0],))\n",
    "        xt, eps_true = q_sample(x0, t, alpha_bars)\n",
    "        t_scaled = (t / T).astype(np.float32).reshape(-1, 1)\n",
    "        eps_hat, cache_for_backward = forward_network(network, np.concatenate([xt, t_scaled], axis=1))\n",
    "        loss, grad = mse_loss(eps_hat, eps_true)\n",
    "        backward_network(network, cache_for_backward, grad, lr=lr)\n",
    "\n",
    "        if running is None:\n",
    "            running = float(loss)\n",
    "        else:\n",
    "            # Momentum tuning\n",
    "            # running = 0.999 * running + 0.001 * float(loss)\n",
    "            running = network[\"momentum\"] * running + (1 - network[\"momentum\"]) * float(loss)\n",
    "\n",
    "        losses.append(running)\n",
    "        if step % 1000 == 0:\n",
    "            print(\"step\" ,step ,\"// smoothed loss MSE=\" , running)\n",
    "    return np.array(losses)\n",
    "\n",
    "losses = train(network, X, T, betas, alphas, alpha_bars, steps=8000, lr=2e-3)\n",
    "\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training loss (steepest descent)\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c17e34",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Sampling: generate new data by reversing diffusion\n",
    "We start from pure noise $x_T \\sim \\mathcal{N}(0, I)$ and go **backwards** to $x_0$ using our network's noise predictions.\n",
    "A simple DDPM step:\n",
    "$$\n",
    "x_{t-1} = \\frac{1}{\\sqrt{\\alpha_t}}\\left(x_t - \\frac{1 - \\alpha_t}{\\sqrt{1 - \\bar{\\alpha}_t}}\\, \\hat{\\epsilon}\\right) + \\sigma_t z, \\quad \\sigma_t = \\sqrt{\\beta_t},\\; z \\sim \\mathcal{N}(0,I).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b096303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def p_sample(network, x_t, t, betas, alphas, alpha_bars):\n",
    "    t_scaled = np.full((x_t.shape[0],1), t/len(betas), dtype=np.float32)\n",
    "    net_in = np.concatenate([x_t, t_scaled], axis=1)\n",
    "    eps_hat, _ = forward_network(network, net_in)\n",
    "\n",
    "    a_t = alphas[t-1]\n",
    "    ab_t = alpha_bars[t-1]\n",
    "    beta_t = betas[t-1]\n",
    "\n",
    "    mean = (1 / np.sqrt(a_t)) * (x_t - ((1 - a_t)/np.sqrt(1 - ab_t)) * eps_hat)\n",
    "\n",
    "    if t > 1:\n",
    "        z = rng.normal(0, 1, size=x_t.shape).astype(np.float32)\n",
    "        sigma = np.sqrt(beta_t)\n",
    "        x_prev = mean + sigma * z\n",
    "    else:\n",
    "        x_prev = mean\n",
    "    return x_prev\n",
    "\n",
    "def p_sample_loop(network, n_samples, T, betas, alphas, alpha_bars,ts_to_show=None):\n",
    "    x_t = rng.normal(0, 1, size=(n_samples, 2)).astype(np.float32)\n",
    "    for t in range(T, 0, -1):\n",
    "        x_t = p_sample(network, x_t, t, betas, alphas, alpha_bars)\n",
    "        if ts_to_show is not None and t in ts_to_show:\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.scatter(X[:,0], X[:,1], s=5, label=\"real\")\n",
    "            plt.scatter(x_t[:,0], x_t[:,1], s=5, label=\"generated\")\n",
    "            plt.title(f\"Generated samples at t={t}\")\n",
    "            plt.xlabel(\"x\")\n",
    "            plt.ylabel(\"y\")\n",
    "            plt.axis(\"equal\")\n",
    "            plt.show()\n",
    "    return x_t\n",
    "\n",
    "gen = p_sample_loop(network, n_samples=1000, T=T, betas=betas, alphas=alphas, alpha_bars=alpha_bars,ts_to_show=ts_to_show)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.scatter(X[:,0], X[:,1], s=5, label=\"real\")\n",
    "plt.scatter(gen[:,0], gen[:,1], s=5, label=\"generated\")\n",
    "plt.legend()\n",
    "plt.title(\"Final Real vs Generated (simple DDPM) at t=0\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Discussion & Experiments\n",
    "Try:\n",
    "- Change **T** (e.g., 50 vs 200).\n",
    "- Try different **beta ranges** (e.g., end=0.01 vs 0.05).\n",
    "- Change **hidden size** (32 or 128).\n",
    "- Observe: training loss, intermediate noisy plots, and final samples.\n",
    "\n",
    "**Questions to reflect on:**\n",
    "- How does the **noise schedule** affect sample quality?\n",
    "- Does a slightly larger network help?\n",
    "- Which plots helped you understand the process the most?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NeuroZero)",
   "language": "python",
   "name": "neurozero"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
